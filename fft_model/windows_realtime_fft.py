#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  Windows FFT TabanlÄ± GerÃ§ek ZamanlÄ± EEG Tahmin
================================================

MindWave'den Raw EEG alÄ±r, filtreleme ve FFT'yi bilgisayarda
hesaplayarak hÄ±zlÄ± tahmin yapar.

âš ï¸ BÄ°LÄ°NEN SORUNLAR:
- CanlÄ± tahmin performansÄ± offline test sonuÃ§larÄ±ndan dÃ¼ÅŸÃ¼k
- Model sÄ±nÄ±flar arasÄ± geÃ§iÅŸlerde zorlanÄ±yor

ğŸ”§ TODO: KALÄ°BRASYON SÄ°STEMÄ° EKLENMELÄ°
Scaler uyumsuzluÄŸu Ã§Ã¶zÃ¼mÃ¼ iÃ§in:
1. Program baÅŸÄ±nda 10-30 sn kalibrasyon
2. KullanÄ±cÄ±nÄ±n nÃ¶tr/dinlenme durumu Ã¶lÃ§Ã¼lecek  
3. KiÅŸiye Ã¶zel mean/std hesaplanacak
4. EÄŸitim scaler'Ä±na oranlanarak adaptif normalizasyon

NeuroSky EEG Power: 1 Hz (saniyede 1 tahmin)
Bu sistem: ~2-4 Hz (saniyede 2-4 tahmin)

KullanÄ±m:
    1. ThinkGear Connector'Ä± baÅŸlatÄ±n
    2. MindWave'i baÄŸlayÄ±n
    3. Bu scripti Ã§alÄ±ÅŸtÄ±rÄ±n:
       python windows_realtime_fft.py

Gereksinimler:
    pip install torch numpy scipy

Ã–zellikler:
    - Raw EEG'den kendi FFT hesaplama
    - Sinyal filtreleme (Notch 50Hz, Bandpass 0.5-50Hz)
    - Artifact rejection
    - CUDA/GPU desteÄŸi (varsa)
    - HÄ±zlÄ± tahmin (~250-500ms aralÄ±klarla)
"""

import os
import sys
import time
import socket
import json
import numpy as np
from collections import deque
from datetime import datetime

# SciPy (filtreleme iÃ§in)
try:
    from scipy import signal as scipy_signal
except ImportError:
    print("âŒ SciPy kurulu deÄŸil!")
    print("   Kurulum: pip install scipy")
    sys.exit(1)

# PyTorch
try:
    import torch
    import torch.nn as nn
except ImportError:
    print("âŒ PyTorch kurulu deÄŸil!")
    print("   Kurulum: pip install torch")
    sys.exit(1)


# ============================================================================
# SÄ°NYAL Ä°ÅLEME PARAMETRELERÄ°
# ============================================================================

SAMPLING_RATE = 512  # Hz
FFT_WINDOW_SIZE = 512  # 1 saniyelik FFT penceresi

# Filtre parametreleri (NeuroSky benzeri)
NOTCH_FREQ = 50      # Hz (TÃ¼rkiye elektrik ÅŸebekesi)
NOTCH_Q = 30         # Notch filter kalite faktÃ¶rÃ¼
LOWCUT = 0.5         # Hz (EEG alt frekans)
HIGHCUT = 50         # Hz (EEG Ã¼st frekans)
FILTER_ORDER = 4     # Butterworth filter order

# Artifact rejection
ARTIFACT_THRESHOLD = 500  # ÂµV Ã¼zeri deÄŸerler artifact sayÄ±lÄ±r

# NeuroSky frekans bantlarÄ± (Hz)
FREQUENCY_BANDS = {
    'Delta': (0.5, 2.75),
    'Theta': (3.5, 6.75),
    'Low Alpha': (7.5, 9.25),
    'High Alpha': (10, 11.75),
    'Low Beta': (13, 16.75),
    'High Beta': (18, 29.75),
    'Low Gamma': (31, 39.75),
    'Mid Gamma': (41, 49.75)
}


# ============================================================================
# SÄ°NYAL FÄ°LTRELEME
# ============================================================================

class SignalProcessor:
    """EEG sinyal iÅŸleme sÄ±nÄ±fÄ±"""
    
    def __init__(self, fs=SAMPLING_RATE):
        self.fs = fs
        
        # Filtreleri Ã¶nceden oluÅŸtur (hÄ±z iÃ§in)
        self.notch_b, self.notch_a = self._create_notch_filter()
        self.bandpass_b, self.bandpass_a = self._create_bandpass_filter()
    
    def _create_notch_filter(self):
        """50 Hz Notch filtre oluÅŸtur"""
        nyq = self.fs / 2
        w0 = NOTCH_FREQ / nyq
        return scipy_signal.iirnotch(w0, NOTCH_Q)
    
    def _create_bandpass_filter(self):
        """Bandpass filtre oluÅŸtur (0.5-50 Hz)"""
        nyq = self.fs / 2
        low = LOWCUT / nyq
        high = HIGHCUT / nyq
        return scipy_signal.butter(FILTER_ORDER, [low, high], btype='band')
    
    def filter_signal(self, raw_samples):
        """
        Raw EEG sinyalini filtrele
        1. DC offset kaldÄ±r
        2. Artifact temizle
        3. Notch filtre (50 Hz)
        4. Bandpass filtre (0.5-50 Hz)
        """
        samples = np.array(raw_samples, dtype=np.float64)
        
        # 1. DC offset kaldÄ±r
        samples = samples - np.mean(samples)
        
        # 2. Artifact'larÄ± temizle
        artifact_mask = np.abs(samples) > ARTIFACT_THRESHOLD
        if np.any(artifact_mask):
            good_samples = samples[~artifact_mask]
            if len(good_samples) > 0:
                median_val = np.median(good_samples)
                samples[artifact_mask] = median_val
        
        # 3. Notch filtre (50 Hz)
        samples = scipy_signal.filtfilt(self.notch_b, self.notch_a, samples)
        
        # 4. Bandpass filtre (0.5-50 Hz)
        samples = scipy_signal.filtfilt(self.bandpass_b, self.bandpass_a, samples)
        
        return samples
    
    def calculate_fft_bands(self, filtered_samples):
        """FFT ile frekans bant gÃ¼Ã§lerini hesapla"""
        samples = np.array(filtered_samples, dtype=np.float64)
        
        # Hamming window
        window = np.hamming(len(samples))
        samples = samples * window
        
        # FFT
        fft_vals = np.abs(np.fft.rfft(samples))
        freqs = np.fft.rfftfreq(len(samples), 1.0 / self.fs)
        
        # GÃ¼Ã§ spektrumu
        power_spectrum = fft_vals ** 2
        
        # Her bant iÃ§in gÃ¼Ã§
        band_powers = []
        for band_name in ['Delta', 'Theta', 'Low Alpha', 'High Alpha', 
                          'Low Beta', 'High Beta', 'Low Gamma', 'Mid Gamma']:
            low_freq, high_freq = FREQUENCY_BANDS[band_name]
            mask = (freqs >= low_freq) & (freqs <= high_freq)
            band_powers.append(np.sum(power_spectrum[mask]))
        
        return band_powers
    
    def process_raw_to_fft(self, raw_samples):
        """Raw EEG â†’ Filtreleme â†’ FFT"""
        filtered = self.filter_signal(raw_samples)
        band_powers = self.calculate_fft_bands(filtered)
        return band_powers


# ============================================================================
# MODEL TANIMLARI
# ============================================================================

class TemporalBlock(nn.Module):
    """TCN iÃ§in Temporal Block - Causal Convolution"""
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()
        self.padding = padding
        
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, 
                               padding=padding, dilation=dilation)
        self.bn1 = nn.BatchNorm1d(n_outputs)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        
        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride,
                               padding=padding, dilation=dilation)
        self.bn2 = nn.BatchNorm1d(n_outputs)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
    
    def forward(self, x):
        out = self.conv1(x)
        out = out[:, :, :x.size(2)]  # Causal: padding'i kes
        out = self.dropout1(self.relu1(self.bn1(out)))
        
        out = self.conv2(out)
        out = out[:, :, :x.size(2)]  # Causal: padding'i kes
        out = self.dropout2(self.relu2(self.bn2(out)))
        
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TCN_Model(nn.Module):
    """Temporal Convolutional Network"""
    def __init__(self, input_channels=9, num_classes=3, num_channels=[64, 128, 256], kernel_size=3, dropout=0.2):
        super(TCN_Model, self).__init__()
        layers = []
        num_levels = len(num_channels)
        
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_channels if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            padding = (kernel_size - 1) * dilation_size
            layers.append(TemporalBlock(in_channels, out_channels, kernel_size, stride=1,
                                        dilation=dilation_size, padding=padding, dropout=dropout))
        
        self.network = nn.Sequential(*layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(num_channels[-1], num_classes)
    
    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.network(x)
        x = self.global_pool(x)
        x = x.squeeze(-1)
        x = self.fc(x)
        return x


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class TransformerModel(nn.Module):
    """Transformer tabanlÄ± model"""
    def __init__(self, input_channels=9, num_classes=3, d_model=64, nhead=4, num_layers=2, dropout=0.1):
        super(TransformerModel, self).__init__()
        
        self.input_projection = nn.Linear(input_channels, d_model)
        self.pos_encoder = nn.Parameter(torch.randn(1, 128, d_model) * 0.1)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, 
                                                    dim_feedforward=d_model*4, 
                                                    dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.fc1 = nn.Linear(d_model, 32)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(32, num_classes)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.input_projection(x)
        x = x + self.pos_encoder[:, :x.size(1), :]
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


class CNN_LSTM_Model(nn.Module):
    def __init__(self, input_channels=9, num_classes=3):
        super(CNN_LSTM_Model, self).__init__()
        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(64)
        self.pool1 = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(128)
        self.pool2 = nn.MaxPool1d(2)
        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm1d(256)
        self.lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.3)
        self.fc1 = nn.Linear(128, 64)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, num_classes)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.pool2(x)
        x = self.relu(self.bn3(self.conv3(x)))
        x = x.permute(0, 2, 1)
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


# ============================================================================
# THINKGEAR BAÄLANTISI
# ============================================================================

class ThinkGearConnector:
    """ThinkGear Connector'a doÄŸrudan baÄŸlanÄ±r ve Raw EEG okur"""
    
    def __init__(self, host='127.0.0.1', port=13854):
        self.host = host
        self.port = port
        self.sock = None
        self.buffer = ""
        
        # Raw EEG buffer
        self.raw_buffer = deque(maxlen=FFT_WINDOW_SIZE * 2)
        
        # Durum
        self.poor_signal = 200
        self.raw_count = 0
    
    def connect(self):
        """ThinkGear Connector'a baÄŸlan"""
        try:
            print(f"ğŸ”µ ThinkGear Connector'a baÄŸlanÄ±lÄ±yor: {self.host}:{self.port}")
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.settimeout(5)
            self.sock.connect((self.host, self.port))
            
            # Raw EEG Ã§Ä±ktÄ±sÄ± iste (512 Hz)
            self.sock.send(b'{"enableRawOutput": true, "format": "Json"}\n')
            
            # TCP optimizasyonlarÄ±
            self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
            self.sock.settimeout(0.05)  # 50ms timeout
            
            print("âœ… BaÄŸlantÄ± baÅŸarÄ±lÄ±!")
            print("ğŸ“¡ Raw EEG Ã§Ä±ktÄ±sÄ±: AKTÄ°F (512 Hz)")
            return True
            
        except ConnectionRefusedError:
            print("âŒ ThinkGear Connector Ã§alÄ±ÅŸmÄ±yor!")
            print("\nğŸ’¡ Ã‡Ã¶zÃ¼m:")
            print("   1. ThinkGear Connector'Ä± baÅŸlatÄ±n")
            print("   2. MindWave cihazÄ±nÄ± baÄŸlayÄ±n")
            print("   3. Bu scripti tekrar Ã§alÄ±ÅŸtÄ±rÄ±n")
            return False
        except Exception as e:
            print(f"âŒ BaÄŸlantÄ± hatasÄ±: {e}")
            return False
    
    def disconnect(self):
        """BaÄŸlantÄ±yÄ± kapat"""
        if self.sock:
            try:
                self.sock.close()
            except:
                pass
        print("ğŸ”Œ BaÄŸlantÄ± kapatÄ±ldÄ±")
    
    def read_data(self):
        """ThinkGear'dan veri oku"""
        if not self.sock:
            return None
        
        try:
            data = self.sock.recv(16384).decode('utf-8')
            if not data:
                return None
            
            self.buffer += data
            
            lines = self.buffer.split('\r')
            self.buffer = lines[-1]
            
            got_raw = False
            for line in lines[:-1]:
                line = line.strip()
                if not line:
                    continue
                
                try:
                    parsed = json.loads(line)
                    
                    # Raw EEG (512 Hz)
                    if 'rawEeg' in parsed:
                        self.raw_buffer.append(parsed['rawEeg'])
                        self.raw_count += 1
                        got_raw = True
                    
                    # Sinyal kalitesi
                    if 'poorSignalLevel' in parsed:
                        self.poor_signal = parsed['poorSignalLevel']
                    
                except json.JSONDecodeError:
                    continue
            
            return 'raw' if got_raw else None
            
        except socket.timeout:
            return None
        except Exception:
            return None
    
    def get_raw_samples(self, n_samples):
        """Son n sample'Ä± al"""
        if len(self.raw_buffer) < n_samples:
            return None
        return list(self.raw_buffer)[-n_samples:]
    
    def get_buffer_size(self):
        return len(self.raw_buffer)


# ============================================================================
# GERÃ‡EK ZAMANLI TAHMÄ°N
# ============================================================================

class WindowsFFTPredictor:
    """Windows'ta FFT tabanlÄ± gerÃ§ek zamanlÄ± tahmin"""
    
    MODELS = {
        '1': ('TCN (%95.70)', TCN_Model, 'tcn_model_fft.pth'),
        '2': ('Transformer (%93.49)', TransformerModel, 'transformer_model_fft.pth'),
        '3': ('CNN-LSTM (%81.57)', CNN_LSTM_Model, 'cnn_lstm_model_fft.pth')
    }
    
    LABELS = ['araba', 'aÅŸaÄŸÄ±', 'yukarÄ±']
    CONFIDENCE_THRESHOLD = 0.70
    
    def __init__(self, model_window=128, fft_window=512, prediction_interval=0.25):
        self.model_window = model_window
        self.fft_window = fft_window
        self.prediction_interval = prediction_interval
        
        # Device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.model_name = None
        
        # Signal processor
        self.signal_processor = SignalProcessor()
        
        # FFT buffer
        self.fft_buffer = deque(maxlen=model_window)
        
        # ThinkGear
        self.thinkgear = ThinkGearConnector()
        
        # Scaler
        self.scaler_mean = None
        self.scaler_std = None
        
        # Stats
        self.predictions = {label: 0 for label in self.LABELS}
        self.total_predictions = 0
        self.uncertain_count = 0
        self.inference_times = []
        self.fft_times = []
    
    def find_model_path(self, filename):
        """Model dosyasÄ±nÄ±n yolunu bul"""
        # OlasÄ± konumlar
        script_dir = os.path.dirname(os.path.abspath(__file__))
        possible_paths = [
            os.path.join(script_dir, filename),
            os.path.join(script_dir, 'fft_model', filename),
            os.path.join(os.path.dirname(script_dir), 'fft_model', filename),
            filename
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def load_scaler_params(self):
        """Scaler parametrelerini yÃ¼kle"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # OlasÄ± konumlar
        possible_paths = [
            os.path.join(script_dir, 'scaler_params_fft.json'),
            os.path.join(script_dir, 'fft_model', 'scaler_params_fft.json'),
            'scaler_params_fft.json'
        ]
        
        for scaler_path in possible_paths:
            if os.path.exists(scaler_path):
                with open(scaler_path, 'r') as f:
                    params = json.load(f)
                self.scaler_mean = np.array(params['mean'])
                self.scaler_std = np.array(params['std'])
                print(f"âœ… Scaler yÃ¼klendi: {scaler_path}")
                return True
        
        print("âš ï¸ Scaler dosyasÄ± bulunamadÄ±")
        return False
    
    def select_model(self):
        """Model seÃ§"""
        print("\n" + "=" * 60)
        print("ğŸ§  FFT MODEL SEÃ‡Ä°MÄ°")
        print("=" * 60)
        
        for key, (name, _, _) in self.MODELS.items():
            print(f"   {key}. {name}")
        
        print("   q. Ã‡Ä±kÄ±ÅŸ")
        print("-" * 60)
        
        while True:
            choice = input("Model seÃ§in (1-3): ").strip()
            
            if choice.lower() == 'q':
                return False
            
            if choice in self.MODELS:
                return self.load_model(choice)
            
            print("âŒ GeÃ§ersiz seÃ§im!")
    
    def load_model(self, choice):
        """Model yÃ¼kle"""
        name, model_class, filename = self.MODELS[choice]
        
        model_path = self.find_model_path(filename)
        if not model_path:
            print(f"âŒ Model bulunamadÄ±: {filename}")
            return False
        
        try:
            print(f"\nğŸ“¥ YÃ¼kleniyor: {name}")
            
            self.model = model_class(input_channels=9, num_classes=3)
            state_dict = torch.load(model_path, map_location=self.device, weights_only=True)
            self.model.load_state_dict(state_dict)
            self.model.to(self.device)
            self.model.eval()
            
            self.model_name = name
            print(f"âœ… Model yÃ¼klendi!")
            print(f"âš¡ Cihaz: {self.device}")
            
            if self.device.type == 'cuda':
                print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def preprocess(self, fft_window_data):
        """FFT verilerini normalize et"""
        x = np.array(fft_window_data, dtype=np.float32)
        
        # Flatten ve normalize
        if self.scaler_mean is not None and self.scaler_std is not None:
            x_flat = x.flatten()
            if len(x_flat) == len(self.scaler_mean):
                x_normalized = (x_flat - self.scaler_mean) / np.where(self.scaler_std > 0, self.scaler_std, 1)
                x = x_normalized.reshape(x.shape)
            else:
                # Fallback: per-channel normalize
                for i in range(x.shape[1]):
                    col = x[:, i]
                    x[:, i] = (col - np.mean(col)) / (np.std(col) + 1e-8)
        
        return torch.FloatTensor(x).unsqueeze(0).to(self.device)
    
    def predict(self, fft_window_data):
        """Tahmin yap"""
        if self.model is None:
            return None, None, 0
        
        start_time = time.time()
        
        with torch.no_grad():
            x = self.preprocess(fft_window_data)
            outputs = self.model(x)
            probs = torch.softmax(outputs, dim=1)
            confidence, predicted = torch.max(probs, 1)
            
            if self.device.type == 'cuda':
                torch.cuda.synchronize()
            
            inference_time = (time.time() - start_time) * 1000
            
            return self.LABELS[predicted.item()], confidence.item(), inference_time
    
    def run(self):
        """Ana dÃ¶ngÃ¼"""
        print("\n" + "=" * 60)
        print("ğŸ§  Windows FFT GerÃ§ek ZamanlÄ± EEG Tahmin")
        print("   Raw EEG â†’ Filtreleme â†’ FFT â†’ Tahmin")
        print("=" * 60)
        
        # Scaler yÃ¼kle
        self.load_scaler_params()
        
        # Model seÃ§
        if not self.select_model():
            return
        
        # BaÄŸlan
        print("\n" + "-" * 60)
        if not self.thinkgear.connect():
            return
        
        print("\n" + "=" * 60)
        print(f"ğŸ“Š Model: {self.model_name}")
        print(f"âš¡ Cihaz: {self.device}")
        print(f"ğŸ¯ SÄ±nÄ±flar: {', '.join(self.LABELS)}")
        print(f"ğŸ“¦ FFT: {self.fft_window} sample (1 saniye)")
        print(f"ğŸ“¦ Model: {self.model_window} frame")
        print(f"â±ï¸ Tahmin: {1/self.prediction_interval:.1f} Hz")
        print("=" * 60)
        print("\nğŸ’¡ MindWave'i takÄ±n!")
        print("â¸ï¸  Durdurmak iÃ§in Ctrl+C")
        print("-" * 60)
        
        try:
            last_prediction_time = 0
            last_raw_count = 0
            raw_samples_for_fft = 256  # Her 256 sample'da FFT
            
            raw_received = False
            prediction_started = False
            
            while True:
                result = self.thinkgear.read_data()
                
                if result == 'raw':
                    raw_received = True
                    
                    # Buffer durumu
                    if not prediction_started:
                        raw_count = self.thinkgear.get_buffer_size()
                        fft_count = len(self.fft_buffer)
                        sig = "âœ…" if self.thinkgear.poor_signal < 50 else f"âš ï¸({self.thinkgear.poor_signal})"
                        print(f"\rğŸ“¦ Raw: {raw_count}/{self.fft_window} | FFT: {fft_count}/{self.model_window} | {sig}   ", end='')
                    
                    current_time = time.time()
                    
                    # FFT hesapla
                    raw_buffer_size = self.thinkgear.get_buffer_size()
                    new_samples = self.thinkgear.raw_count - last_raw_count
                    
                    if raw_buffer_size >= self.fft_window and new_samples >= raw_samples_for_fft:
                        last_raw_count = self.thinkgear.raw_count
                        
                        fft_start = time.time()
                        raw_samples = self.thinkgear.get_raw_samples(self.fft_window)
                        band_powers = self.signal_processor.process_raw_to_fft(raw_samples)
                        fft_time = (time.time() - fft_start) * 1000
                        self.fft_times.append(fft_time)
                        
                        # FFT buffer'a ekle [Electrode=0, Delta, Theta, ...]
                        self.fft_buffer.append([0] + band_powers)
                    
                    # Tahmin zamanÄ±
                    if len(self.fft_buffer) >= self.model_window and (current_time - last_prediction_time) >= self.prediction_interval:
                        last_prediction_time = current_time
                        prediction_started = True
                        
                        fft_data = list(self.fft_buffer)[-self.model_window:]
                        label, confidence, inf_time = self.predict(fft_data)
                        
                        self.inference_times.append(inf_time)
                        
                        if label:
                            avg_fft = sum(self.fft_times[-10:]) / min(len(self.fft_times), 10) if self.fft_times else 0
                            
                            print()
                            print("\n" + "=" * 60)
                            
                            if confidence >= self.CONFIDENCE_THRESHOLD:
                                self.predictions[label] += 1
                                self.total_predictions += 1
                                
                                print(f"â° {datetime.now().strftime('%H:%M:%S')} | #{self.total_predictions}")
                                print(f"âš¡ FFT: {avg_fft:.1f}ms | Model: {inf_time:.1f}ms")
                                print(f"ğŸ¯ {label.upper()} ({confidence*100:.1f}%)")
                            else:
                                self.uncertain_count += 1
                                
                                print(f"â° {datetime.now().strftime('%H:%M:%S')} | â“ Belirsiz #{self.uncertain_count}")
                                print(f"âš¡ FFT: {avg_fft:.1f}ms | Model: {inf_time:.1f}ms")
                                print(f"ğŸ¤” {label} ({confidence*100:.1f}% < {self.CONFIDENCE_THRESHOLD*100:.0f}%)")
                            
                            print("-" * 60)
                            
                            for l in self.LABELS:
                                count = self.predictions[l]
                                total = max(self.total_predictions, 1)
                                pct = count / total * 100
                                bar = "â–ˆ" * int(pct / 5)
                                marker = "ğŸ‘‰" if (l == label and confidence >= self.CONFIDENCE_THRESHOLD) else "  "
                                print(f"{marker} {l:8}: {bar:<20} {pct:.1f}%")
                            
                            if self.uncertain_count > 0:
                                all_total = self.total_predictions + self.uncertain_count
                                u_pct = self.uncertain_count / all_total * 100
                                print(f"   {'belirsiz':8}: {'â–‘' * int(u_pct / 5):<20} {u_pct:.1f}%")
                            
                            print("=" * 60)
                
                elif not raw_received:
                    print("\râ³ Raw EEG bekleniyor...", end='')
                
                time.sleep(0.001)
                
        except KeyboardInterrupt:
            print("\n\nâ›” Durduruldu")
        finally:
            self.thinkgear.disconnect()
            self._print_summary()
    
    def _print_summary(self):
        """Ã–zet"""
        if self.total_predictions > 0:
            avg_inf = sum(self.inference_times) / len(self.inference_times) if self.inference_times else 0
            avg_fft = sum(self.fft_times) / len(self.fft_times) if self.fft_times else 0
            
            print("\n" + "=" * 60)
            print("ğŸ“Š Ã–ZET")
            print("=" * 60)
            print(f"Toplam tahmin: {self.total_predictions}")
            print(f"Belirsiz: {self.uncertain_count}")
            print(f"Ortalama FFT: {avg_fft:.2f}ms")
            print(f"Ortalama model: {avg_inf:.2f}ms")
            print(f"Toplam gecikme: {avg_fft + avg_inf:.2f}ms")
            print()
            for label in self.LABELS:
                count = self.predictions[label]
                pct = count / self.total_predictions * 100 if self.total_predictions > 0 else 0
                print(f"   {label}: {count} ({pct:.1f}%)")
            print("=" * 60)


def main():
    print("\n" + "=" * 60)
    print("ğŸ§  Windows FFT GerÃ§ek ZamanlÄ± EEG Tahmin")
    print("=" * 60)
    
    # CUDA
    if torch.cuda.is_available():
        print(f"âœ… CUDA: {torch.cuda.get_device_name(0)}")
    else:
        print("â„¹ï¸ CPU kullanÄ±lacak")
    
    print("\nğŸ“‹ Gereksinimler:")
    print("   1. ThinkGear Connector Ã§alÄ±ÅŸÄ±yor")
    print("   2. MindWave baÄŸlÄ±")
    
    print("\nğŸš€ Avantajlar:")
    print("   - NeuroSky 1 Hz â†’ Bu sistem ~2-4 Hz")
    print("   - Kendi filtreleme")
    print("   - %95.70 doÄŸruluk (TCN)")
    
    predictor = WindowsFFTPredictor(
        model_window=128,
        fft_window=512,
        prediction_interval=0.25
    )
    predictor.run()


if __name__ == "__main__":
    main()
