# Transformer Model

Modern attention mechanism kullanan Transformer mimarisi. Az veri iÃ§in optimize edilmiÅŸ kÃ¼Ã§Ã¼k versiyonu.

## ğŸ“‹ Model Ã–zellikleri

**Mimari:**
- Self-Attention Mechanism
- Positional Encoding
- Multi-Head Attention (4 heads)
- Feedforward Neural Network
- Layer Normalization
- Residual Connections

**Optimizasyon (Az Veri Ä°Ã§in):**
- KÃ¼Ã§Ã¼k model boyutu (d_model=64)
- Az katman (2 encoder layer)
- Az attention head (4 head)
- YÃ¼ksek dropout (0.3)
- Gradient clipping

## ğŸš€ KullanÄ±m

```bash
cd /home/kadir/sanal-makine/python/proje/model_experiments/Transformer
python3 transformer_model.py
```

## âš™ï¸ Hiperparametreler

- **Batch Size:** 32
- **Epochs:** 50
- **Learning Rate:** 0.001
- **Optimizer:** Adam
- **d_model:** 64 (embedding dimension)
- **nhead:** 4 (attention heads)
- **num_layers:** 2 (encoder layers)
- **dim_feedforward:** 256
- **Dropout:** 0.3

## ğŸ“Š Model YapÄ±sÄ±

```
Input (batch, 128, 9)
    â†“
Input Projection (9â†’64)
    â†“
Positional Encoding
    â†“
Transformer Encoder Layer 1
  â”œâ”€ Multi-Head Self-Attention (4 heads)
  â”œâ”€ Add & Norm
  â”œâ”€ Feedforward (64â†’256â†’64)
  â””â”€ Add & Norm
    â†“
Transformer Encoder Layer 2
  â””â”€ (same structure)
    â†“
Global Average Pooling
    â†“
FC (64â†’128)
    â†“
Dropout (0.3)
    â†“
FC (128â†’3)
    â†“
Output (3 classes)
```

## ğŸ“ˆ Beklenen Performans

| Metrik | Tahmini DeÄŸer |
|--------|---------------|
| Toplam Parametreler | ~50,000-100,000 |
| Test Accuracy | %60-80 (az veri) |
| EÄŸitim SÃ¼resi | ~3-5 dakika |

**Not:** Transformer modelleri genellikle daha fazla veri gerektirir (50k+ Ã¶rnek). 14k Ã¶rnekle sÄ±nÄ±rlÄ± performans beklenir.

## âš ï¸ Az Veri Problemi

**Neden dÃ¼ÅŸÃ¼k performans olabilir?**
- Transformer'lar veri aÃ§lÄ±ÄŸÄ± Ã§eker
- Self-attention Ã§ok fazla parametre Ã¶ÄŸrenir
- 14k Ã¶rnek ideal deÄŸil (50k+ Ã¶nerilir)

**AlÄ±nan Ã¶nlemler:**
1. âœ… KÃ¼Ã§Ã¼k model boyutu (d_model=64)
2. âœ… Az katman (2 layer)
3. âœ… YÃ¼ksek dropout (0.3)
4. âœ… Gradient clipping
5. âœ… Learning rate scheduling

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

- `transformer_best_model.pth` - En iyi validation accuracy
- `transformer_final_model.pth` - Son epoch modeli
- `transformer_training_history.png` - Loss ve accuracy grafikleri
- `transformer_confusion_matrix.png` - Test seti confusion matrix
- `transformer_training_log.txt` - DetaylÄ± eÄŸitim raporu

## ğŸ¯ Mini Tahmin Testi

EÄŸitim sonunda 10 rastgele Ã¶rnek Ã¼zerinde test yapÄ±lÄ±r.

**AyrÄ±ca test etmek iÃ§in:**
```bash
cd ..
python3 mini_test.py Transformer
```

## ğŸ’¡ Ne Zaman Transformer KullanÄ±lmalÄ±?

**âœ… Transformer iyidir eÄŸer:**
- Ã‡ok fazla veri varsa (50k+ Ã¶rnek)
- Uzun vadeli baÄŸÄ±mlÄ±lÄ±klar varsa
- Paralel iÅŸlem Ã¶nemliyse
- SOTA performans gerekiyorsa

**âŒ Transformer kÃ¶tÃ¼dÃ¼r eÄŸer:**
- Az veri varsa (<20k Ã¶rnek) â† Bizim durum
- HÄ±zlÄ± inference gerekiyorsa
- KÃ¼Ã§Ã¼k model isteniyorsa
- Basit pattern'ler yeterliyse

## ğŸ”— Ä°lgili Modeller

- [TCN](../TCN/) - Ã–nerilen! (%92.44 accuracy) â­
- [EEGNet](../EGGnet/) - EEG Ã¶zel ama veri uyumsuz
- CNN+LSTM - Ana projede

## ğŸ“ Notlar

- Bu implementasyon az veri iÃ§in optimize edilmiÅŸtir
- Daha fazla veriyle (data augmentation) performans artabilir
- TCN bu veri miktarÄ± iÃ§in daha uygun
- Transformer'Ä±n gÃ¼cÃ¼nÃ¼ gÃ¶rmek iÃ§in 50k+ Ã¶rnek gerekir

## ğŸ§ª Deneysel SonuÃ§lar

EÄŸitim tamamlandÄ±ÄŸÄ±nda buraya eklenecek:
- Test Accuracy: ?
- Mini Test: ?/10
- SÄ±nÄ±f PerformanslarÄ±: ?
