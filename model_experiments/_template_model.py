#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MODEL_ADI - [Model aÃ§Ä±klamasÄ±]

Bu dosya yeni model eklemek iÃ§in template olarak kullanÄ±labilir.
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime
import time

# ========================================
# KONFIGURASYON - HER MODEL Ä°Ã‡Ä°N DEÄÄ°ÅTÄ°R
# ========================================
DATA_DIR = "/home/kadir/sanal-makine/python/proje"
SAVE_DIR = "/home/kadir/sanal-makine/python/proje/model_experiments/MODEL_KLASORU"  # DEÄÄ°ÅTÄ°R!
MODEL_NAME = "model_adi"  # DEÄÄ°ÅTÄ°R! (Ã¶rn: "tcn", "eegnet", "transformer")
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ========================================
# MODEL TANIMI - BURAYA KENDÄ° MODELÄ°NÄ° YAZ
# ========================================
class YourModel(nn.Module):
    """
    Kendi model mimarinizi buraya tanÄ±mlayÄ±n
    """
    def __init__(self, input_channels=9, num_classes=3, samples=128):
        super(YourModel, self).__init__()
        
        # Model katmanlarÄ±nÄ± buraya ekle
        # Ã–rnek:
        # self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3)
        # self.fc = nn.Linear(hidden_size, num_classes)
        
        pass
    
    def forward(self, x):
        """
        Forward pass
        Input: (batch, seq_len, features)
        Output: (batch, num_classes)
        """
        # Forward pass mantÄ±ÄŸÄ±nÄ± buraya yaz
        pass


# ========================================
# VERÄ° YÃœKLEME VE HAZIRLIK
# ========================================
def load_data():
    """Ã–nceden iÅŸlenmiÅŸ X ve y verilerini yÃ¼kle"""
    print("\n" + "="*70)
    print("VERÄ° YÃœKLEME")
    print("="*70)
    
    X = np.load(os.path.join(DATA_DIR, 'X.npy'))
    y = np.load(os.path.join(DATA_DIR, 'y.npy'))
    
    with open(os.path.join(DATA_DIR, 'label_map.json'), 'r') as f:
        label_map = json.load(f)
    
    print(f"âœ“ X shape: {X.shape}")
    print(f"âœ“ y shape: {y.shape}")
    print(f"âœ“ SÄ±nÄ±flar: {label_map}")
    print(f"âœ“ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
    unique, counts = np.unique(y, return_counts=True)
    for label, count in zip(unique, counts):
        label_name = [k for k, v in label_map.items() if v == label][0]
        print(f"   - {label_name} ({label}): {count} Ã¶rnek")
    
    return X, y, label_map


def prepare_dataloaders(X, y, test_size=0.2, val_size=0.1):
    """Veriyi train, validation ve test setlerine ayÄ±r"""
    print("\n" + "="*70)
    print("VERÄ° SETLERÄ° HAZIRLANIYOR")
    print("="*70)
    
    # Ã–nce train+val ve test'e ayÄ±r
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )
    
    # Sonra train+val'i train ve val'e ayÄ±r
    val_ratio = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_ratio, random_state=42, stratify=y_temp
    )
    
    print(f"âœ“ EÄŸitim seti: {X_train.shape[0]} Ã¶rnek ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"âœ“ DoÄŸrulama seti: {X_val.shape[0]} Ã¶rnek ({X_val.shape[0]/len(X)*100:.1f}%)")
    print(f"âœ“ Test seti: {X_test.shape[0]} Ã¶rnek ({X_test.shape[0]/len(X)*100:.1f}%)")
    
    # PyTorch tensorlerine Ã§evir
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.LongTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.LongTensor(y_val)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.LongTensor(y_test)
    
    # DataLoader oluÅŸtur
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    return train_loader, val_loader, test_loader, (X_test, y_test)


# ========================================
# EÄÄ°TÄ°M VE DOÄRULAMA
# ========================================
def train_epoch(model, train_loader, criterion, optimizer):
    """Tek epoch eÄŸitim"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc


def validate_epoch(model, val_loader, criterion):
    """Tek epoch doÄŸrulama"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc


# ========================================
# TEST VE DEÄERLENDÄ°RME
# ========================================
def test_model(model, test_loader, label_map):
    """Model performansÄ±nÄ± test setinde deÄŸerlendir"""
    print("\n" + "="*70)
    print("TEST SETÄ° DEÄERLENDÄ°RME")
    print("="*70)
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.numpy())
    
    # Accuracy hesapla
    correct = sum([1 for p, l in zip(all_predictions, all_labels) if p == l])
    accuracy = 100 * correct / len(all_labels)
    
    print(f"\nâœ“ Test Accuracy: {accuracy:.2f}%")
    
    # Classification report
    label_names = [k for k, v in sorted(label_map.items(), key=lambda x: x[1])]
    print("\n" + "-"*70)
    print("Classification Report:")
    print("-"*70)
    print(classification_report(all_labels, all_predictions, target_names=label_names))
    
    # Confusion matrix
    cm = confusion_matrix(all_labels, all_predictions)
    
    return accuracy, cm, label_names


def mini_prediction_test(model, X_test, y_test, label_map, num_samples=10):
    """
    ğŸ¯ MÄ°NÄ° TAHMÄ°N TESTÄ°
    EÄŸitim sonrasÄ± 10 Ã¶rneklik gerÃ§ek zamanlÄ± tahmin testi
    Her yeni model iÃ§in ZORUNLU!
    """
    print("\n" + "="*70)
    print("ğŸ¯ MÄ°NÄ° TAHMÄ°N TESTÄ° (10 Ã–rnek)")
    print("="*70)
    
    model.eval()
    
    # Random 10 Ã¶rnek seÃ§
    indices = np.random.choice(len(X_test), num_samples, replace=False)
    
    # Reverse label map (id -> name)
    id_to_label = {v: k for k, v in label_map.items()}
    
    correct_count = 0
    
    print("\n{:<5} {:<15} {:<15} {:<10}".format("No", "GerÃ§ek", "Tahmin", "SonuÃ§"))
    print("-" * 50)
    
    with torch.no_grad():
        for i, idx in enumerate(indices, 1):
            # Tek Ã¶rnek al
            sample = torch.FloatTensor(X_test[idx:idx+1]).to(DEVICE)
            true_label = y_test[idx]
            
            # Tahmin yap
            output = model(sample)
            probabilities = F.softmax(output, dim=1)
            predicted_id = torch.argmax(output, dim=1).item()
            confidence = probabilities[0, predicted_id].item() * 100
            
            # Label isimlerini al
            true_name = id_to_label[true_label]
            pred_name = id_to_label[predicted_id]
            
            # DoÄŸru mu kontrol et
            is_correct = true_label == predicted_id
            result = "âœ“ DOÄRU" if is_correct else "âœ— YANLIÅ"
            
            if is_correct:
                correct_count += 1
            
            print("{:<5} {:<15} {:<15} {} ({:.1f}%)".format(
                i, true_name, pred_name, result, confidence
            ))
    
    print("-" * 50)
    print(f"Mini Test Accuracy: {correct_count}/{num_samples} ({correct_count*10}%)")
    print("="*70)


# ========================================
# GRAFÄ°KLER VE RAPORLAMA
# ========================================
def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path):
    """EÄŸitim grafiklerini Ã§iz"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # Loss grafiÄŸi
    ax1.plot(epochs, train_losses, 'b-', label='EÄŸitim Loss', linewidth=2, marker='o', markersize=4)
    ax1.plot(epochs, val_losses, 'r-', label='DoÄŸrulama Loss', linewidth=2, marker='s', markersize=4)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title(f'{MODEL_NAME.upper()} Model - Loss DeÄŸiÅŸimi', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # Accuracy grafiÄŸi
    ax2.plot(epochs, train_accs, 'b-', label='EÄŸitim Accuracy', linewidth=2, marker='o', markersize=4)
    ax2.plot(epochs, val_accs, 'r-', label='DoÄŸrulama Accuracy', linewidth=2, marker='s', markersize=4)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title(f'{MODEL_NAME.upper()} Model - Accuracy DeÄŸiÅŸimi', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ“ EÄŸitim grafikleri kaydedildi: {os.path.basename(save_path)}")
    plt.close()


def plot_confusion_matrix(cm, label_names, save_path):
    """Confusion matrix Ã§iz"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, 
                yticklabels=label_names, cbar_kws={'label': 'Ã–rnek SayÄ±sÄ±'})
    plt.title(f'{MODEL_NAME.upper()} Model - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
    plt.ylabel('GerÃ§ek SÄ±nÄ±f', fontsize=12)
    plt.xlabel('Tahmin Edilen SÄ±nÄ±f', fontsize=12)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ Confusion matrix kaydedildi: {os.path.basename(save_path)}")
    plt.close()


def save_training_log(log_data, save_path):
    """EÄŸitim logunu kaydet"""
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write(f"{MODEL_NAME.upper()} MODEL EÄÄ°TÄ°M LOGU\n")
        f.write("="*70 + "\n\n")
        f.write(f"Tarih: {log_data['timestamp']}\n")
        f.write(f"Device: {log_data['device']}\n\n")
        
        f.write("MODEL YAPISI:\n")
        f.write("-"*70 + "\n")
        f.write(f"Model: {log_data['model_name']}\n")
        f.write(f"Toplam Parametreler: {log_data['total_params']:,}\n")
        f.write(f"EÄŸitilebilir Parametreler: {log_data['trainable_params']:,}\n\n")
        
        f.write("HÄ°PERPARAMETRELER:\n")
        f.write("-"*70 + "\n")
        f.write(f"Batch Size: {log_data['batch_size']}\n")
        f.write(f"Epochs: {log_data['epochs']}\n")
        f.write(f"Learning Rate: {log_data['learning_rate']}\n")
        f.write(f"Optimizer: {log_data['optimizer']}\n\n")
        
        f.write("SONUÃ‡LAR:\n")
        f.write("-"*70 + "\n")
        f.write(f"En Ä°yi DoÄŸrulama Accuracy: {log_data['best_val_acc']:.2f}%\n")
        f.write(f"Final Test Accuracy: {log_data['test_acc']:.2f}%\n")
        f.write(f"Toplam EÄŸitim SÃ¼resi: {log_data['training_time']:.2f} saniye\n\n")
        
        f.write("DETAYLI EPOCH LOGU:\n")
        f.write("-"*70 + "\n")
        for i, (tl, ta, vl, va) in enumerate(zip(log_data['train_losses'], 
                                                  log_data['train_accs'],
                                                  log_data['val_losses'],
                                                  log_data['val_accs']), 1):
            f.write(f"Epoch {i:3d} | Train Loss: {tl:.4f} | Train Acc: {ta:.2f}% | "
                   f"Val Loss: {vl:.4f} | Val Acc: {va:.2f}%\n")
    
    print(f"âœ“ EÄŸitim logu kaydedildi: {os.path.basename(save_path)}")


# ========================================
# MAIN FONKSÄ°YON
# ========================================
def main():
    start_time = time.time()
    
    print("\n" + "="*70)
    print(f"{MODEL_NAME.upper()} MODEL EÄÄ°TÄ°MÄ°")
    print("="*70)
    print(f"Device: {DEVICE}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA Version: {torch.version.cuda}")
    
    # Veri yÃ¼kleme
    X, y, label_map = load_data()
    num_classes = len(label_map)
    num_channels = X.shape[2]
    samples = X.shape[1]
    
    # Data loaders
    train_loader, val_loader, test_loader, (X_test, y_test) = prepare_dataloaders(X, y)
    
    # Model oluÅŸtur
    print("\n" + "="*70)
    print("MODEL OLUÅTURULUYOR")
    print("="*70)
    
    model = YourModel(
        input_channels=num_channels,
        num_classes=num_classes,
        samples=samples
    ).to(DEVICE)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ“ Model oluÅŸturuldu")
    print(f"âœ“ Toplam parametreler: {total_params:,}")
    print(f"âœ“ EÄŸitilebilir parametreler: {trainable_params:,}")
    
    # Loss ve optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # EÄŸitim
    print("\n" + "="*70)
    print(f"EÄÄ°TÄ°M BAÅLIYOR - {EPOCHS} EPOCH")
    print("="*70)
    
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    best_val_acc = 0.0
    
    for epoch in range(EPOCHS):
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)
        val_loss, val_acc = validate_epoch(model, val_loader, criterion)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        scheduler.step(val_loss)
        
        # Progress bar
        bar_length = 30
        progress = (epoch + 1) / EPOCHS
        filled = int(bar_length * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        print(f"[{bar}] Epoch {epoch+1:3d}/{EPOCHS} | "
              f"Train: {train_acc:6.2f}% | Val: {val_acc:6.2f}% | Loss: {val_loss:.4f}", end='')
        
        # En iyi model kaydet
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, f'{MODEL_NAME}_best_model.pth'))
            print(" âœ“ BEST", end='')
        
        print()
    
    # Final model kaydet
    torch.save(model.state_dict(), os.path.join(SAVE_DIR, f'{MODEL_NAME}_final_model.pth'))
    
    # Test seti deÄŸerlendirmesi
    test_acc, cm, label_names = test_model(model, test_loader, label_map)
    
    # ğŸ¯ Mini Tahmin Testi
    # Not: EÄŸitim sonrasÄ± hÄ±zlÄ± kontrol iÃ§in
    # DetaylÄ± test iÃ§in: python3 ../mini_test.py MODEL_KLASORU
    mini_prediction_test(model, X_test, y_test, label_map, num_samples=10)
    
    # Grafikleri kaydet
    plot_training_history(train_losses, val_losses, train_accs, val_accs,
                         os.path.join(SAVE_DIR, f'{MODEL_NAME}_training_history.png'))
    
    plot_confusion_matrix(cm, label_names,
                         os.path.join(SAVE_DIR, f'{MODEL_NAME}_confusion_matrix.png'))
    
    # EÄŸitim logu kaydet
    training_time = time.time() - start_time
    log_data = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'device': str(DEVICE),
        'model_name': MODEL_NAME.upper(),
        'total_params': total_params,
        'trainable_params': trainable_params,
        'batch_size': BATCH_SIZE,
        'epochs': EPOCHS,
        'learning_rate': LEARNING_RATE,
        'optimizer': 'Adam',
        'best_val_acc': best_val_acc,
        'test_acc': test_acc,
        'training_time': training_time,
        'train_losses': train_losses,
        'train_accs': train_accs,
        'val_losses': val_losses,
        'val_accs': val_accs
    }
    
    save_training_log(log_data, os.path.join(SAVE_DIR, f'{MODEL_NAME}_training_log.txt'))
    
    # Ã–zet
    print("\n" + "="*70)
    print("EÄÄ°TÄ°M TAMAMLANDI! ğŸ‰")
    print("="*70)
    print(f"âœ“ En iyi doÄŸrulama accuracy: {best_val_acc:.2f}%")
    print(f"âœ“ Test accuracy: {test_acc:.2f}%")
    print(f"âœ“ Toplam sÃ¼re: {training_time:.2f} saniye ({training_time/60:.2f} dakika)")
    print(f"\nKaydedilen dosyalar ({SAVE_DIR}):")
    print(f"  ğŸ“ {MODEL_NAME}_best_model.pth")
    print(f"  ğŸ“ {MODEL_NAME}_final_model.pth")
    print(f"  ğŸ“Š {MODEL_NAME}_training_history.png")
    print(f"  ğŸ“Š {MODEL_NAME}_confusion_matrix.png")
    print(f"  ğŸ“ {MODEL_NAME}_training_log.txt")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
