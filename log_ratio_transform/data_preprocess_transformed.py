#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Log Transform + Oran FormÃ¼lleri ile Veri Ã–n Ä°ÅŸleme (FFT Verileri)

Bu script FFT hesaplanmÄ±ÅŸ verilere:
1. Log Transform (log1p) uygular
2. Basit Oran FormÃ¼lleri ekler (8 yeni Ã¶zellik)

AmaÃ§: FFT bant gÃ¼Ã§lerindeki kÃ¼Ã§Ã¼k farklarÄ± bÃ¼yÃ¼tmek
Performans YÃ¼kÃ¼: %0.05 (pratik 0)

KAYNAK: ../fft_model/data/ veya ../fft_model/data_filtered/
"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import json
from pathlib import Path

# ============================================================================
# AYARLAR
# ============================================================================
SCRIPT_DIR = Path(__file__).parent
FFT_MODEL_DIR = SCRIPT_DIR.parent / "fft_model"

# FiltrelenmiÅŸ veri varsa onu kullan, yoksa normal FFT veriyi
DATA_DIR_FILTERED = FFT_MODEL_DIR / "data_filtered"
DATA_DIR_NORMAL = FFT_MODEL_DIR / "data"
DATA_DIR = DATA_DIR_FILTERED if DATA_DIR_FILTERED.exists() else DATA_DIR_NORMAL

OUTPUT_DIR = SCRIPT_DIR  # log_ratio_transform/

# FFT ile hesaplanmÄ±ÅŸ 9 Ã¶zellik (fft_model/data/ Ã§Ä±ktÄ±sÄ±)
EEG_FEATURES = ["Electrode", "Delta", "Theta", "Low Alpha", "High Alpha", 
                "Low Beta", "High Beta", "Low Gamma", "Mid Gamma"]

# Yeni oran Ã¶zellikleri (8 tane)
RATIO_NAMES = [
    "Delta_Theta",      # Delta / Theta
    "Theta_Alpha",      # Theta / Alpha
    "Alpha_Beta",       # Alpha / Beta
    "Beta_Gamma",       # Beta / Gamma
    "Slow_Fast",        # (Theta + Alpha) / (Beta + Gamma)
    "Delta_Alpha",      # Delta / Alpha
    "VeryLow_High",     # (Delta + Theta) / (Alpha + Beta + Gamma)
    "Mid_Low",          # (Alpha + Beta) / (Delta + Theta)
]

WINDOW_SIZE = 128
OVERLAP = 64
START_EVENT = 33025
END_EVENT = 33024

# ============================================================================
# TRANSFORMASYON FONKSÄ°YONLARI
# ============================================================================

def apply_log_transform(data):
    """
    Log transform uygula: log1p(x) = log(1 + x)
    BÃ¼yÃ¼k deÄŸerlerdeki kÃ¼Ã§Ã¼k farklarÄ± vurgular
    Negatif deÄŸerler iÃ§in: sign(x) * log1p(|x|)
    """
    return np.sign(data) * np.log1p(np.abs(data))

def calculate_band_ratios(window):
    """
    8 oran Ã¶zelliÄŸi hesapla (her frame iÃ§in)
    
    Input: (128, 9) - 128 frame, 9 Ã¶zellik
    Output: (128, 8) - 128 frame, 8 oran
    """
    # Bant indeksleri (EEG_FEATURES sÄ±rasÄ±na gÃ¶re)
    # 0: Electrode, 1: Delta, 2: Theta, 3: Low Alpha, 4: High Alpha
    # 5: Low Beta, 6: High Beta, 7: Low Gamma, 8: Mid Gamma
    
    delta = window[:, 1] + 1e-8
    theta = window[:, 2] + 1e-8
    low_alpha = window[:, 3] + 1e-8
    high_alpha = window[:, 4] + 1e-8
    low_beta = window[:, 5] + 1e-8
    high_beta = window[:, 6] + 1e-8
    low_gamma = window[:, 7] + 1e-8
    mid_gamma = window[:, 8] + 1e-8
    
    # Kombine bantlar
    alpha = (low_alpha + high_alpha) / 2
    beta = (low_beta + high_beta) / 2
    gamma = (low_gamma + mid_gamma) / 2
    
    # 8 oran hesapla
    ratios = np.column_stack([
        delta / theta,                          # Delta_Theta
        theta / alpha,                          # Theta_Alpha
        alpha / beta,                           # Alpha_Beta
        beta / gamma,                           # Beta_Gamma
        (theta + alpha) / (beta + gamma),       # Slow_Fast
        delta / alpha,                          # Delta_Alpha
        (delta + theta) / (alpha + beta + gamma),  # VeryLow_High
        (alpha + beta) / (delta + theta),       # Mid_Low
    ])
    
    return ratios

def transform_window(window):
    """
    Tek bir window'a tÃ¼m transformasyonlarÄ± uygula
    
    Input: (128, 9)
    Output: (128, 17) - 9 orijinal (log transformed) + 8 oran
    """
    # 1. Log transform uygula
    log_transformed = apply_log_transform(window)
    
    # 2. OranlarÄ± hesapla (orijinal veriden, log'dan deÄŸil)
    ratios = calculate_band_ratios(window)
    
    # 3. Log transform'u oranlara da uygula
    ratios_log = apply_log_transform(ratios)
    
    # 4. BirleÅŸtir
    combined = np.hstack([log_transformed, ratios_log])
    
    return combined

# ============================================================================
# VERÄ° YÃœKLEME VE Ä°ÅLEME
# ============================================================================

def load_csv_files():
    """
    fft_model/data veya fft_model/data_filtered klasÃ¶rÃ¼ndeki
    FFT hesaplanmÄ±ÅŸ CSV dosyalarÄ±nÄ± yÃ¼kle
    """
    csv_files = []
    
    if not DATA_DIR.exists():
        print(f"âŒ Veri dizini bulunamadÄ±: {DATA_DIR}")
        return csv_files
    
    # Kategori klasÃ¶rlerini tara
    for category_dir in sorted(DATA_DIR.iterdir()):
        if not category_dir.is_dir():
            continue
        
        class_name = category_dir.name
        
        # Kategori dÃ¼zeltmeleri
        if class_name == "asagÄ±":
            class_name = "aÅŸaÄŸÄ±"
        
        # Bu kategorideki CSV dosyalarÄ±nÄ± yÃ¼kle
        for csv_file in sorted(category_dir.glob("*.csv")):
            try:
                df = pd.read_csv(csv_file)
                csv_files.append((csv_file.name, df, class_name))
                print(f"âœ… YÃ¼klendi: {category_dir.name}/{csv_file.name} â†’ {class_name} ({len(df)} satÄ±r)")
            except Exception as e:
                print(f"âŒ Hata ({csv_file.name}): {e}")
    
    return csv_files

def extract_active_segments(df):
    """
    Event Id sÃ¼tunundaki START/END iÅŸaretlerini kullanarak
    sadece aktif (dÃ¼ÅŸÃ¼nme) bÃ¶lgelerini Ã§Ä±kar
    
    Returns:
        list of DataFrames: Aktif segmentler
    """
    active_segments = []
    
    if 'Event Id' not in df.columns:
        # Event Id yoksa tÃ¼m veriyi dÃ¶ndÃ¼r
        print("      âš  Event Id sÃ¼tunu yok, tÃ¼m veri kullanÄ±lacak")
        return [df]
    
    # Event Id'leri sayÄ±sal deÄŸerlere Ã§evir (NaN'larÄ± 0 yap)
    event_ids = pd.to_numeric(df['Event Id'], errors='coerce').fillna(0).astype(int)
    
    # BaÅŸlangÄ±Ã§ ve bitiÅŸ indekslerini bul
    start_indices = df.index[event_ids == START_EVENT].tolist()
    end_indices = df.index[event_ids == END_EVENT].tolist()
    
    if not start_indices:
        print("      âš  START iÅŸareti bulunamadÄ±, tÃ¼m veri kullanÄ±lacak")
        return [df]
    
    print(f"      ğŸ“ {len(start_indices)} START, {len(end_indices)} END iÅŸareti bulundu")
    
    # Her START iÃ§in en yakÄ±n END'i bul
    for start_idx in start_indices:
        # Bu START'tan sonraki END'leri bul
        valid_ends = [end for end in end_indices if end > start_idx]
        if valid_ends:
            end_idx = valid_ends[0]
            # START ve END arasÄ±ndaki veriyi al
            segment = df.iloc[start_idx:end_idx+1].copy()
            if len(segment) > WINDOW_SIZE:
                active_segments.append(segment)
                print(f"      âœ… Aktif segment: {len(segment)} satÄ±r ({len(segment)/512:.1f}s)")
    
    if not active_segments:
        print("      âš  Aktif segment bulunamadÄ±")
    
    return active_segments

def create_windows_with_transform(segment):
    """Segment'ten window'lar oluÅŸtur ve transform uygula"""
    data = segment[EEG_FEATURES].values
    data = np.nan_to_num(data, nan=0.0)
    
    windows = []
    step = WINDOW_SIZE - OVERLAP
    
    for i in range(0, len(data) - WINDOW_SIZE + 1, step):
        window = data[i:i + WINDOW_SIZE]  # (128, 9)
        
        # Transform uygula
        transformed = transform_window(window)  # (128, 17)
        windows.append(transformed)
    
    return np.array(windows) if windows else np.array([])

def process_all_data(csv_files):
    """TÃ¼m verileri iÅŸle - sadece aktif bÃ¶lgeleri kullan"""
    all_windows = []
    all_labels = []
    label_map = {}
    current_label = 0
    
    for filename, df, class_name in csv_files:
        print(f"\n{'='*60}")
        print(f"ğŸ“‚ Ä°ÅŸleniyor: {filename} â†’ {class_name}")
        
        if class_name not in label_map:
            label_map[class_name] = current_label
            current_label += 1
        
        label = label_map[class_name]
        segments = extract_active_segments(df)
        
        for seg_idx, segment in enumerate(segments):
            windows = create_windows_with_transform(segment)
            if len(windows) > 0:
                all_windows.append(windows)
                all_labels.extend([label] * len(windows))
                print(f"   âœ… Segment {seg_idx+1}: {len(windows)} pencere")
    
    if all_windows:
        X = np.vstack(all_windows)
        y = np.array(all_labels)
        return X, y, label_map
    return None, None, None

# ============================================================================
# ANA FONKSÄ°YON
# ============================================================================

def main():
    print("\n" + "=" * 70)
    print("ğŸ§ª LOG TRANSFORM + ORAN FORMÃœLLERÄ° DENEMESÄ°")
    print("=" * 70)
    
    print("\nğŸ“‹ Uygulanan Transformasyonlar:")
    print("   1. Log Transform: log1p(x) = log(1 + x)")
    print("   2. Oran FormÃ¼lleri: 8 yeni Ã¶zellik")
    print(f"   â†’ Girdi: 9 Ã¶zellik â†’ Ã‡Ä±ktÄ±: 17 Ã¶zellik")
    
    print("\nğŸ“‚ Veri kaynaÄŸÄ±:")
    exists = "âœ…" if DATA_DIR.exists() else "âŒ"
    print(f"   {exists} {DATA_DIR}")
    if DATA_DIR == DATA_DIR_FILTERED:
        print(f"   (FiltrelenmiÅŸ FFT verileri kullanÄ±lÄ±yor)")
    else:
        print(f"   (Normal FFT verileri kullanÄ±lÄ±yor)")
    
    # Verileri yÃ¼kle
    csv_files = load_csv_files()
    if not csv_files:
        print("\nâŒ CSV dosyasÄ± bulunamadÄ±!")
        return
    
    # Verileri iÅŸle
    X, y, label_map = process_all_data(csv_files)
    if X is None:
        print("\nâŒ Veri iÅŸleme baÅŸarÄ±sÄ±z!")
        return
    
    print(f"\n{'='*70}")
    print("ğŸ“Š SONUÃ‡LAR")
    print("=" * 70)
    print(f"ğŸ“¦ X shape: {X.shape}")
    print(f"   â†’ Orijinal: (N, 128, 9)")
    print(f"   â†’ Yeni:     (N, 128, 17)")
    print(f"ğŸ“¦ y shape: {y.shape}")
    print(f"ğŸ·ï¸  Label map: {label_map}")
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
    print(f"\nğŸ“Š SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
    reverse_map = {v: k for k, v in label_map.items()}
    for label_idx in sorted(reverse_map.keys()):
        count = np.sum(y == label_idx)
        pct = (count / len(y)) * 100
        print(f"   {reverse_map[label_idx]:10s}: {count:5d} ({pct:.1f}%)")
    
    # Normalizasyon
    print(f"\nğŸ”„ Normalizasyon...")
    original_shape = X.shape
    X_flat = X.reshape(X.shape[0], -1)
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X_flat).reshape(original_shape)
    print(f"   Mean: {X_normalized.mean():.6f}")
    print(f"   Std:  {X_normalized.std():.6f}")
    
    # Kaydet
    print(f"\nğŸ’¾ Kaydediliyor...")
    np.save(os.path.join(OUTPUT_DIR, 'X_transformed.npy'), X_normalized)
    np.save(os.path.join(OUTPUT_DIR, 'y_transformed.npy'), y)
    with open(os.path.join(OUTPUT_DIR, 'label_map_transformed.json'), 'w', encoding='utf-8') as f:
        json.dump(label_map, f, indent=2, ensure_ascii=False)
    
    # Scaler'Ä± da kaydet
    import pickle
    with open(os.path.join(OUTPUT_DIR, 'scaler_transformed.pkl'), 'wb') as f:
        pickle.dump(scaler, f)
    
    print(f"   âœ… X_transformed.npy")
    print(f"   âœ… y_transformed.npy")
    print(f"   âœ… label_map_transformed.json")
    print(f"   âœ… scaler_transformed.pkl")
    
    # FFT verileriyle karÅŸÄ±laÅŸtÄ±r
    print(f"\n{'='*70}")
    print("ğŸ“Š FFT vs TRANSFORMED KARÅILAÅTIRMASI")
    print("=" * 70)
    
    try:
        X_fft = np.load(FFT_MODEL_DIR / 'X_fft.npy')
        print(f"   FFT X shape:        {X_fft.shape}")
        print(f"   Transformed X shape: {X.shape}")
        print(f"   Ã–zellik artÄ±ÅŸÄ±: {X.shape[2] - X_fft.shape[2]} Ã¶zellik (+{((X.shape[2] / X_fft.shape[2]) - 1) * 100:.0f}%)")
    except:
        print("   (FFT verileri bulunamadÄ±)")
    
    print(f"\n{'='*70}")
    print("âœ… TAMAMLANDI!")
    print("=" * 70)
    print("\nğŸ“Œ Sonraki adÄ±m: Bu veriyle yeni model eÄŸit")
    print(f"ğŸ“Œ Komut: python3 train_model_transformed.py")
    print("   python3 train_model_transformed.py")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()
