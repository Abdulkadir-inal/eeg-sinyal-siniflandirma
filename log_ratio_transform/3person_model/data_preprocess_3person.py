#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Log Transform + Oran FormÃ¼lleri ile Veri Ã–n Ä°ÅŸleme (3 KiÅŸi: Apo, BahadÄ±r, Canan)

Bu script FFT hesaplanmÄ±ÅŸ verilere:
1. Log Transform (log1p) uygular
2. Basit Oran FormÃ¼lleri ekler (8 yeni Ã¶zellik)

SADECE APO, BAHADIR ve CANAN verileri kullanÄ±lÄ±r!

KAYNAK: ../../fft_model/data_filtered/
Ã‡IKTI: 3person_model/
"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import json
from pathlib import Path

# ============================================================================
# AYARLAR
# ============================================================================
SCRIPT_DIR = Path(__file__).parent
FFT_MODEL_DIR = SCRIPT_DIR.parent.parent / "fft_model"

# FiltrelenmiÅŸ veri varsa onu kullan, yoksa normal FFT veriyi
DATA_DIR_FILTERED = FFT_MODEL_DIR / "data_filtered"
DATA_DIR_NORMAL = FFT_MODEL_DIR / "data"
DATA_DIR = DATA_DIR_FILTERED if DATA_DIR_FILTERED.exists() else DATA_DIR_NORMAL

OUTPUT_DIR = SCRIPT_DIR  # 3person_model/

# SADECE BU 3 KÄ°ÅÄ°NÄ°N VERÄ°LERÄ° KULLANILACAK
ALLOWED_PERSONS = ["apo", "bahadÄ±r", "canan"]

# FFT ile hesaplanmÄ±ÅŸ 9 Ã¶zellik (fft_model/data/ Ã§Ä±ktÄ±sÄ±)
EEG_FEATURES = ["Electrode", "Delta", "Theta", "Low Alpha", "High Alpha", 
                "Low Beta", "High Beta", "Low Gamma", "Mid Gamma"]

# Yeni oran Ã¶zellikleri (8 tane)
RATIO_NAMES = [
    "Delta_Theta",      # Delta / Theta
    "Theta_Alpha",      # Theta / Alpha
    "Alpha_Beta",       # Alpha / Beta
    "Beta_Gamma",       # Beta / Gamma
    "Slow_Fast",        # (Theta + Alpha) / (Beta + Gamma)
    "Delta_Alpha",      # Delta / Alpha
    "VeryLow_High",     # (Delta + Theta) / (Alpha + Beta + Gamma)
    "Mid_Low",          # (Alpha + Beta) / (Delta + Theta)
]

WINDOW_SIZE = 128
OVERLAP = 64
START_EVENT = 33025
END_EVENT = 33024

# ============================================================================
# TRANSFORMASYON FONKSÄ°YONLARI
# ============================================================================

def apply_log_transform(data):
    """
    Log transform uygula: log1p(x) = log(1 + x)
    BÃ¼yÃ¼k deÄŸerlerdeki kÃ¼Ã§Ã¼k farklarÄ± vurgular
    Negatif deÄŸerler iÃ§in: sign(x) * log1p(|x|)
    """
    return np.sign(data) * np.log1p(np.abs(data))

def calculate_band_ratios(window):
    """
    8 oran Ã¶zelliÄŸi hesapla (her frame iÃ§in)
    
    Input: (128, 9) - 128 frame, 9 Ã¶zellik
    Output: (128, 8) - 128 frame, 8 oran
    """
    delta = window[:, 1] + 1e-8
    theta = window[:, 2] + 1e-8
    low_alpha = window[:, 3] + 1e-8
    high_alpha = window[:, 4] + 1e-8
    low_beta = window[:, 5] + 1e-8
    high_beta = window[:, 6] + 1e-8
    low_gamma = window[:, 7] + 1e-8
    mid_gamma = window[:, 8] + 1e-8
    
    # Kombine bantlar
    alpha = (low_alpha + high_alpha) / 2
    beta = (low_beta + high_beta) / 2
    gamma = (low_gamma + mid_gamma) / 2
    
    # 8 oran hesapla
    ratios = np.column_stack([
        delta / theta,                          # Delta_Theta
        theta / alpha,                          # Theta_Alpha
        alpha / beta,                           # Alpha_Beta
        beta / gamma,                           # Beta_Gamma
        (theta + alpha) / (beta + gamma),       # Slow_Fast
        delta / alpha,                          # Delta_Alpha
        (delta + theta) / (alpha + beta + gamma),  # VeryLow_High
        (alpha + beta) / (delta + theta),       # Mid_Low
    ])
    
    return ratios

def transform_window(window):
    """
    Tek bir window'a tÃ¼m transformasyonlarÄ± uygula
    
    Input: (128, 9)
    Output: (128, 17) - 9 orijinal (log transformed) + 8 oran
    """
    # 1. Log transform uygula
    log_transformed = apply_log_transform(window)
    
    # 2. OranlarÄ± hesapla (orijinal veriden, log'dan deÄŸil)
    ratios = calculate_band_ratios(window)
    
    # 3. Log transform'u oranlara da uygula
    ratios_log = apply_log_transform(ratios)
    
    # 4. BirleÅŸtir
    combined = np.hstack([log_transformed, ratios_log])
    
    return combined

# ============================================================================
# VERÄ° YÃœKLEME VE Ä°ÅLEME
# ============================================================================

def load_csv_files():
    """
    fft_model/data_filtered klasÃ¶rÃ¼ndeki FFT hesaplanmÄ±ÅŸ CSV dosyalarÄ±nÄ± yÃ¼kle
    SADECE APO, BAHADIR, CANAN dosyalarÄ±!
    """
    csv_files = []
    
    if not DATA_DIR.exists():
        print(f"âŒ Veri dizini bulunamadÄ±: {DATA_DIR}")
        return csv_files
    
    print(f"\nğŸ” SADECE {', '.join(ALLOWED_PERSONS).upper()} verileri yÃ¼kleniyor...")
    
    # Kategori klasÃ¶rlerini tara
    for category_dir in sorted(DATA_DIR.iterdir()):
        if not category_dir.is_dir():
            continue
        
        class_name = category_dir.name
        
        # Kategori dÃ¼zeltmeleri
        if class_name == "asagÄ±":
            class_name = "aÅŸaÄŸÄ±"
        
        # Bu kategorideki CSV dosyalarÄ±nÄ± yÃ¼kle
        for csv_file in sorted(category_dir.glob("*.csv")):
            # Dosya adÄ±ndan kiÅŸiyi Ã§Ä±kar
            filename = csv_file.stem.lower()
            
            # Ã–nce canan_annane'yi exclude et (canan_ kontrolÃ¼nden Ã¶nce!)
            if "annane" in filename or "Ä±rmak" in filename:
                print(f"â­ï¸  AtlandÄ±: {category_dir.name}/{csv_file.name} (hariÃ§ tutulan kiÅŸi)")
                continue
            
            # KiÅŸi kontrolÃ¼ (dosya adÄ± kiÅŸi ismi ile baÅŸlamalÄ±)
            person_found = False
            for person in ALLOWED_PERSONS:
                # Dosya ismi "kiÅŸi_" ile baÅŸlamalÄ± (Ã¶rn: "apo_", "bahadÄ±r_", "canan_")
                if filename.startswith(person + "_"):
                    person_found = True
                    break
            
            if not person_found:
                print(f"â­ï¸  AtlandÄ±: {category_dir.name}/{csv_file.name} (izinli kiÅŸi deÄŸil)")
                continue
            
            try:
                df = pd.read_csv(csv_file)
                csv_files.append((csv_file.name, df, class_name))
                print(f"âœ… YÃ¼klendi: {category_dir.name}/{csv_file.name} â†’ {class_name} ({len(df)} satÄ±r)")
            except Exception as e:
                print(f"âŒ Hata ({csv_file.name}): {e}")
    
    return csv_files

def extract_active_segments(df):
    """
    Event Id sÃ¼tunundaki START/END iÅŸaretlerini kullanarak
    sadece aktif (dÃ¼ÅŸÃ¼nme) bÃ¶lgelerini Ã§Ä±kar
    
    Returns:
        list of DataFrames: Aktif segmentler
    """
    active_segments = []
    
    if 'Event Id' not in df.columns:
        print("      âš  Event Id sÃ¼tunu yok, tÃ¼m veri kullanÄ±lacak")
        return [df]
    
    # Event Id'leri sayÄ±sal deÄŸerlere Ã§evir (NaN'larÄ± 0 yap)
    event_ids = pd.to_numeric(df['Event Id'], errors='coerce').fillna(0).astype(int)
    
    # BaÅŸlangÄ±Ã§ ve bitiÅŸ indekslerini bul
    start_indices = df.index[event_ids == START_EVENT].tolist()
    end_indices = df.index[event_ids == END_EVENT].tolist()
    
    if not start_indices:
        print("      âš  START iÅŸareti bulunamadÄ±, tÃ¼m veri kullanÄ±lacak")
        return [df]
    
    print(f"      ğŸ“ {len(start_indices)} START, {len(end_indices)} END iÅŸareti bulundu")
    
    # Her START iÃ§in en yakÄ±n END'i bul
    for start_idx in start_indices:
        valid_ends = [end for end in end_indices if end > start_idx]
        if valid_ends:
            end_idx = valid_ends[0]
            segment = df.iloc[start_idx:end_idx].copy()
            if len(segment) > 0:
                active_segments.append(segment)
    
    if not active_segments:
        print("      âš  Aktif segment bulunamadÄ±, tÃ¼m veri kullanÄ±lacak")
        return [df]
    
    return active_segments

def create_windows(features, window_size=WINDOW_SIZE, overlap=OVERLAP):
    """
    Kayan pencere ile window'lar oluÅŸtur
    
    Input:
        features: (N, 9) numpy array
    Output:
        windows: (M, window_size, 9) numpy array
    """
    stride = window_size - overlap
    windows = []
    
    for i in range(0, len(features) - window_size + 1, stride):
        window = features[i:i+window_size]
        windows.append(window)
    
    return np.array(windows)

# ============================================================================
# ANA Ä°ÅLEM
# ============================================================================

def main():
    print("\n" + "=" * 80)
    print("ğŸ§  LOG TRANSFORM + ORAN FORMÃœLLERÄ° - VERÄ° Ã–N Ä°ÅLEME (3 KÄ°ÅÄ°)")
    print("=" * 80)
    print(f"ğŸ“‚ Veri Dizini: {DATA_DIR}")
    print(f"ğŸ“ Ã‡Ä±ktÄ± Dizini: {OUTPUT_DIR}")
    print(f"ğŸ‘¥ Ä°zinli KiÅŸiler: {', '.join(ALLOWED_PERSONS).upper()}")
    print(f"ğŸªŸ Window: {WINDOW_SIZE} frame, Overlap: {OVERLAP}")
    print("-" * 80)
    
    # 1. CSV dosyalarÄ±nÄ± yÃ¼kle
    csv_files = load_csv_files()
    
    if not csv_files:
        print("\nâŒ HiÃ§ veri yÃ¼klenemedi!")
        return
    
    print(f"\nâœ… Toplam {len(csv_files)} dosya yÃ¼klendi (sadece apo, bahadÄ±r, canan)")
    
    # 2. Window'larÄ± oluÅŸtur
    all_windows = []
    all_labels = []
    
    label_map = {"araba": 0, "aÅŸaÄŸÄ±": 1, "yukarÄ±": 2}
    
    for filename, df, class_name in csv_files:
        print(f"\nğŸ”„ Ä°ÅŸleniyor: {filename} â†’ {class_name}")
        
        # Aktif segmentleri Ã§Ä±kar
        segments = extract_active_segments(df)
        print(f"      ğŸ“¦ {len(segments)} aktif segment bulundu")
        
        for seg_idx, segment in enumerate(segments, 1):
            # Ã–zellik sÃ¼tunlarÄ±nÄ± al
            features = segment[EEG_FEATURES].values
            
            if len(features) < WINDOW_SIZE:
                print(f"      âš  Segment {seg_idx} Ã§ok kÄ±sa ({len(features)} < {WINDOW_SIZE}), atlanÄ±yor")
                continue
            
            # Window'larÄ± oluÅŸtur
            windows = create_windows(features)
            
            if len(windows) == 0:
                continue
            
            all_windows.extend(windows)
            all_labels.extend([label_map[class_name]] * len(windows))
            
            print(f"      âœ… Segment {seg_idx}: {len(features)} frame â†’ {len(windows)} window")
    
    if not all_windows:
        print("\nâŒ HiÃ§ window oluÅŸturulamadÄ±!")
        return
    
    # 3. Numpy array'e Ã§evir
    X = np.array(all_windows, dtype=np.float32)  # (N, 128, 9)
    y = np.array(all_labels, dtype=np.int64)     # (N,)
    
    print(f"\nğŸ“Š Ham Veri:")
    print(f"   X shape: {X.shape}")
    print(f"   y shape: {y.shape}")
    print(f"   SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
    for class_name, label_idx in label_map.items():
        count = np.sum(y == label_idx)
        print(f"      {class_name:8s}: {count:5d} ({count/len(y)*100:.1f}%)")
    
    # 4. Log Transform + Oran FormÃ¼lleri uygula
    print(f"\nğŸ”„ Transform uygulanÄ±yor...")
    X_transformed = []
    
    for i, window in enumerate(X):
        transformed = transform_window(window)  # (128, 9) â†’ (128, 17)
        X_transformed.append(transformed)
        
        if (i + 1) % 1000 == 0:
            print(f"   {i+1}/{len(X)} window iÅŸlendi...")
    
    X_transformed = np.array(X_transformed, dtype=np.float32)
    
    print(f"\nâœ… Transform tamamlandÄ±!")
    print(f"   X_transformed shape: {X_transformed.shape}")
    print(f"   Ã–zellikler: 9 FFT + 8 Oran = 17 toplam")
    
    # 5. Normalizasyon (StandardScaler)
    print(f"\nğŸ”„ StandardScaler uygulanÄ±yor...")
    
    # Window'larÄ± flat'le: (N, 128, 17) â†’ (N, 128*17)
    n_samples = X_transformed.shape[0]
    X_flat = X_transformed.reshape(n_samples, -1)
    
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X_flat)
    
    # Tekrar reshape: (N, 128*17) â†’ (N, 128, 17)
    X_final = X_normalized.reshape(n_samples, WINDOW_SIZE, 17)
    
    print(f"   âœ… Normalizasyon tamamlandÄ±")
    
    # 6. Dosyalara kaydet
    print(f"\nğŸ’¾ Dosyalar kaydediliyor...")
    
    X_path = OUTPUT_DIR / "X_3person.npy"
    y_path = OUTPUT_DIR / "y_3person.npy"
    scaler_path = OUTPUT_DIR / "scaler_3person.pkl"
    label_map_path = OUTPUT_DIR / "label_map_3person.json"
    
    np.save(X_path, X_final)
    np.save(y_path, y)
    
    import pickle
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    with open(label_map_path, 'w') as f:
        json.dump(label_map, f, indent=2)
    
    print(f"   âœ… X_3person.npy kaydedildi: {X_final.shape}")
    print(f"   âœ… y_3person.npy kaydedildi: {y.shape}")
    print(f"   âœ… scaler_3person.pkl kaydedildi")
    print(f"   âœ… label_map_3person.json kaydedildi")
    
    # 7. Ã–zet istatistikler
    print("\n" + "=" * 80)
    print("ğŸ“Š Ã–ZET Ä°STATÄ°STÄ°KLER (3 KÄ°ÅÄ°)")
    print("=" * 80)
    print(f"ğŸ‘¥ KullanÄ±lan KiÅŸiler: {', '.join(ALLOWED_PERSONS).upper()}")
    print(f"ğŸ“ Toplam dosya: {len(csv_files)}")
    print(f"ğŸªŸ Toplam window: {len(X_final)}")
    print(f"ğŸ”¢ Ã–zellik sayÄ±sÄ±: {X_final.shape[2]} (9 FFT + 8 Oran)")
    print(f"ğŸ“ Window boyutu: {WINDOW_SIZE} frame")
    print(f"\nğŸ¯ SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:")
    for class_name, label_idx in label_map.items():
        count = np.sum(y == label_idx)
        print(f"   {class_name:8s}: {count:5d} ({count/len(y)*100:.1f}%)")
    print("=" * 80)


if __name__ == "__main__":
    main()
